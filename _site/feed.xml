<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2026-01-12T17:32:44+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">K8sReady</title><subtitle>Infraestructura Cloud moderna, segura y eficiente</subtitle><entry xml:lang="es"><title type="html">Exponiendo URLs internas y externas en GKE sin mezclar responsabilidades</title><link href="http://localhost:4000/2026/01/12/exponiendo-urls-internas-y-externas-en-gke-es/" rel="alternate" type="text/html" title="Exponiendo URLs internas y externas en GKE sin mezclar responsabilidades" /><published>2026-01-12T00:00:00+01:00</published><updated>2026-01-12T00:00:00+01:00</updated><id>http://localhost:4000/2026/01/12/exponiendo-urls-internas-y-externas-en-gke-es</id><content type="html" xml:base="http://localhost:4000/2026/01/12/exponiendo-urls-internas-y-externas-en-gke-es/"><![CDATA[<p>Cuando diseñas una arquitectura en <strong>GCP con Kubernetes</strong>, una de las decisiones más importantes es <strong>cómo exponer tus URLs internas y externas sin mezclar responsabilidades ni comprometer seguridad</strong>.</p>

<p>No es un problema de “qué funciona”, sino de <strong>qué modelo mental quieres imponer a tu plataforma</strong>.</p>

<h2 id="urls-internas-accesibles-solo-desde-tu-red">URLs internas: accesibles solo desde tu red</h2>

<p>Para servicios internos (backoffice, operaciones, APIs privadas), el patrón más limpio es usar un <strong>Internal Load Balancer</strong>.</p>

<p>En este modelo:</p>

<ul>
  <li>El servicio solo tiene <strong>IP privada</strong></li>
  <li>El acceso se realiza desde:
    <ul>
      <li>VPN</li>
      <li>red corporativa</li>
      <li>workloads internos en la VPC</li>
    </ul>
  </li>
  <li>No existe ningún endpoint público alcanzable desde Internet</li>
</ul>

<p>En <strong>GKE</strong>, esto suele materializarse como:</p>

<ul>
  <li>Service o Ingress <strong>interno</strong></li>
  <li>Traefik (u otro ingress) expuesto únicamente mediante un <strong>Internal Load Balancer</strong></li>
  <li>Firewall y routing de red proporcionando el aislamiento real</li>
</ul>

<p>La ventaja clave es clara:</p>

<blockquote>
  <p>Lo interno <strong>no existe</strong> para Internet, ni siquiera por error de configuración de aplicación.</p>
</blockquote>

<p>Aquí el aislamiento no depende de headers, rutas o middlewares, sino de <strong>red</strong>.</p>

<h2 id="urls-externas-expuestas-de-forma-controlada">URLs externas: expuestas de forma controlada</h2>

<p>Para servicios públicos (webs, APIs públicas), el patrón habitual es un <strong>External HTTP(S) Load Balancer</strong>.</p>

<p>Un diseño muy común y efectivo en GKE es:</p>

<ul>
  <li>External Load Balancer (con <strong>Cloud Armor</strong> si aplica)</li>
  <li>El load balancer enruta tráfico hacia <strong>NEGs de GKE</strong></li>
  <li>Los NEGs apuntan directamente a pods de <strong>Traefik</strong></li>
  <li>Traefik se encarga del routing a los servicios dentro del clúster</li>
</ul>

<p>Puntos importantes de este enfoque:</p>

<ul>
  <li>Traefik <strong>no necesita IP pública propia</strong></li>
  <li>El único punto accesible desde Internet es el Load Balancer</li>
  <li>El acceso a los pods está limitado a la infraestructura de Google (health checks / proxies)</li>
  <li>No existen NodePorts ni Load Balancers adicionales abiertos</li>
</ul>

<p>Este patrón reduce complejidad y funciona muy bien como <strong>edge público</strong>, manteniendo el control en el perímetro.</p>

<h2 id="un-solo-gateway-dos-caminos-de-entrada">Un solo gateway, dos caminos de entrada</h2>

<p>Aunque ambos patrones usen <strong>Traefik</strong> y <strong>Kubernetes</strong>, el objetivo de cada uno es distinto:</p>

<ul>
  <li><strong>Interno</strong>: aislamiento por red</li>
  <li><strong>Externo</strong>: exposición controlada por perímetro (LB + WAF)</li>
</ul>

<p>Ambos flujos convergen en el mismo punto lógico dentro del clúster:</p>

<ul>
  <li>El tráfico interno llega a Traefik a través de un <strong>Internal Load Balancer</strong></li>
  <li>El tráfico externo llega a Traefik a través de <strong>NEGs desde un External Load Balancer</strong></li>
</ul>

<p>El backend final es el mismo: <strong>los microservicios del clúster</strong>.<br />
Lo que cambia es <strong>cómo</strong> se llega hasta ellos.</p>

<h2 id="la-decisión-real">La decisión real</h2>

<p>Puedes:</p>

<ul>
  <li>Usar Traefik para URLs privadas</li>
  <li>Usar Traefik para URLs públicas</li>
  <li>Compartir backend services si tiene sentido</li>
</ul>

<p>Pero nunca deberías <strong>confundir los puntos de entrada</strong>.</p>

<p>La decisión no es “qué funciona”, sino:</p>

<blockquote>
  <p>qué nivel de aislamiento quieres y dónde colocas tu perímetro real.</p>
</blockquote>

<p>Diseñar bien esta separación desde el principio ahorra complejidad, reduce riesgo operativo y hace que la arquitectura sea mucho más fácil de razonar a largo plazo.</p>

<p>Si estás diseñando o evolucionando una plataforma en GKE y quieres comentar estos patrones, puedes <a href="/es/contact">contactar conmigo</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Diseñar correctamente cómo exponer servicios internos y externos en Kubernetes es clave para mantener aislamiento, seguridad y claridad operativa en GCP.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/load-balancers.png" /><media:content medium="image" url="http://localhost:4000/assets/img/load-balancers.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry xml:lang="en"><title type="html">Exposing Internal and External URLs in GKE Without Mixing Responsibilities</title><link href="http://localhost:4000/2026/01/12/gke-internal-external-urls-en/" rel="alternate" type="text/html" title="Exposing Internal and External URLs in GKE Without Mixing Responsibilities" /><published>2026-01-12T00:00:00+01:00</published><updated>2026-01-12T00:00:00+01:00</updated><id>http://localhost:4000/2026/01/12/gke-internal-external-urls-en</id><content type="html" xml:base="http://localhost:4000/2026/01/12/gke-internal-external-urls-en/"><![CDATA[<p>When designing an architecture in <strong>GCP with Kubernetes</strong>, one of the most important decisions is <strong>how to expose internal and external URLs without mixing responsibilities or compromising security</strong>.</p>

<p>This is not about “what works”, but about <strong>what mental model you want to enforce in your platform</strong>.</p>

<h2 id="internal-urls-accessible-only-from-your-network">Internal URLs: accessible only from your network</h2>

<p>For internal services (backoffice, operations, private APIs), the cleanest pattern is to use an <strong>Internal Load Balancer</strong>.</p>

<p>In this model:</p>

<ul>
  <li>The service has <strong>only a private IP</strong></li>
  <li>Access is limited to:
    <ul>
      <li>VPN</li>
      <li>corporate network</li>
      <li>internal workloads within the VPC</li>
    </ul>
  </li>
  <li>There is no public endpoint reachable from the Internet</li>
</ul>

<p>In <strong>GKE</strong>, this is usually implemented as:</p>

<ul>
  <li>An <strong>internal</strong> Service or Ingress</li>
  <li>Traefik (or another ingress controller) exposed only through an <strong>Internal Load Balancer</strong></li>
  <li>Firewall rules and network routing providing real isolation</li>
</ul>

<p>The key advantage is clear:</p>

<blockquote>
  <p>Internal services <strong>do not exist</strong> for the Internet, not even due to application misconfiguration.</p>
</blockquote>

<p>Here, isolation does not rely on headers, paths, or middlewares, but on <strong>network boundaries</strong>.</p>

<h2 id="external-urls-exposed-in-a-controlled-way">External URLs: exposed in a controlled way</h2>

<p>For public services (websites, public APIs), the standard pattern is an <strong>External HTTP(S) Load Balancer</strong>.</p>

<p>A very common and effective design in GKE is:</p>

<ul>
  <li>External Load Balancer (with <strong>Cloud Armor</strong>, if applicable)</li>
  <li>The load balancer routes traffic to <strong>GKE NEGs</strong></li>
  <li>NEGs point directly to <strong>Traefik pods</strong></li>
  <li>Traefik handles routing to services inside the cluster</li>
</ul>

<p>Important characteristics of this approach:</p>

<ul>
  <li>Traefik <strong>does not need its own public IP</strong></li>
  <li>The only Internet-facing component is the Load Balancer</li>
  <li>Pod access is restricted to Google infrastructure (health checks / proxies)</li>
  <li>No NodePorts or additional Load Balancers are exposed</li>
</ul>

<p>This pattern reduces complexity and works very well as a <strong>public edge</strong>, keeping control at the perimeter.</p>

<h2 id="one-gateway-two-entry-paths">One gateway, two entry paths</h2>

<p>Although both patterns use <strong>Traefik</strong> and <strong>Kubernetes</strong>, their goals are different:</p>

<ul>
  <li><strong>Internal</strong>: isolation through networking</li>
  <li><strong>External</strong>: controlled exposure at the perimeter (LB + WAF)</li>
</ul>

<p>Both traffic flows converge at the same logical point inside the cluster:</p>

<ul>
  <li>Internal traffic reaches Traefik through an <strong>Internal Load Balancer</strong></li>
  <li>External traffic reaches Traefik through <strong>NEGs from an External Load Balancer</strong></li>
</ul>

<p>The final backend is the same: <strong>the cluster’s microservices</strong>.<br />
What changes is <strong>how</strong> traffic reaches them.</p>

<h2 id="the-real-decision">The real decision</h2>

<p>You can:</p>

<ul>
  <li>Use Traefik for private URLs</li>
  <li>Use Traefik for public URLs</li>
  <li>Share backend services when it makes sense</li>
</ul>

<p>But you should never <strong>mix entry points</strong>.</p>

<p>The decision is not “what works”, but:</p>

<blockquote>
  <p>what level of isolation you want and where you place your real perimeter.</p>
</blockquote>

<p>Designing this separation correctly from the beginning reduces complexity, lowers operational risk, and makes the architecture much easier to reason about over time.</p>

<p>If you are designing or evolving a GKE platform and want to discuss these patterns, you can <a href="/en/contact">contact me</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Designing how internal and external services are exposed in Kubernetes is key to preserving isolation, security, and operational clarity in GCP.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/load-balancers.png" /><media:content medium="image" url="http://localhost:4000/assets/img/load-balancers.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry xml:lang="es"><title type="html">Cilium en EKS vs GKE: por qué el networking en Kubernetes se siente más simple en GKE</title><link href="http://localhost:4000/2026/01/08/cilium-networking-gke-vs-eks-es/" rel="alternate" type="text/html" title="Cilium en EKS vs GKE: por qué el networking en Kubernetes se siente más simple en GKE" /><published>2026-01-08T00:00:00+01:00</published><updated>2026-01-08T00:00:00+01:00</updated><id>http://localhost:4000/2026/01/08/cilium-networking-gke-vs-eks-es</id><content type="html" xml:base="http://localhost:4000/2026/01/08/cilium-networking-gke-vs-eks-es/"><![CDATA[<p>Tras desplegar <strong>Cilium</strong> tanto en <strong>EKS</strong> como en <strong>GKE</strong>, hubo algo que se volvió muy evidente:</p>

<p><strong>El networking es mucho más fácil de razonar en GKE.</strong></p>

<p>No se trata de qué plataforma es “mejor”, sino de qué tan alineado está su modelo de red con la forma en la que Kubernetes fue diseñado originalmente.</p>

<h2 id="el-networking-de-kubernetes-como-fue-concebido-gke">El networking de Kubernetes como fue concebido (GKE)</h2>

<p>En GKE, el modelo de red de Kubernetes se siente natural y predecible. La mayoría de las cosas se comportan como esperas si entiendes los fundamentos de Kubernetes.</p>

<p>Algunas características clave:</p>

<ul>
  <li>Los pods obtienen IPs desde <strong>rangos secundarios de la VPC</strong></li>
  <li><strong>IPAM puede funcionar en modo <code class="language-plaintext highlighter-rouge">kubernetes</code></strong> sin fricción</li>
  <li>Las IPs de los pods son <strong>ciudadanos de primera clase</strong> dentro de la VPC</li>
  <li>La densidad de pods y el escalado son predecibles</li>
  <li>Las funcionalidades avanzadas de Cilium funcionan con mínimos ajustes específicos de la plataforma</li>
</ul>

<p>Desde el punto de vista operativo, esto reduce mucho la carga mental. Piensas en términos de Kubernetes, no en limitaciones de infraestructura.</p>

<h2 id="donde-eks-añade-fricción">Donde EKS añade fricción</h2>

<p>EKS es una plataforma sólida, pero su networking está más acoplado a primitivas de infraestructura propias de AWS.</p>

<p>En la práctica, esto introduce varias restricciones adicionales:</p>

<ul>
  <li>Las IPs de los pods están ligadas a <strong>ENIs</strong></li>
  <li>El agotamiento de IPs requiere <strong>muchísima más planificación previa</strong></li>
  <li>La densidad de pods depende del tipo de instancia EC2</li>
  <li>Algunas funcionalidades de Cilium requieren cambiar el <strong>tipo de target del Load Balancer</strong> (instance vs IP)</li>
  <li>El modelo mental pasa a ser <strong>infraestructura-céntrico</strong>, no Kubernetes-céntrico</li>
</ul>

<p>Nada de esto convierte a EKS en una mala plataforma. Pero sí implica que operar configuraciones avanzadas de red requiere un mayor conocimiento de los detalles internos de AWS y más disciplina operativa.</p>

<h2 id="cilium-amplifica-las-diferencias">Cilium amplifica las diferencias</h2>

<p>Cilium no es el problema. De hecho, funciona muy bien en ambas plataformas.</p>

<p>Pero precisamente porque Cilium expone y se apoya en conceptos de networking nativos de Kubernetes, <strong>las diferencias entre plataformas se hacen más visibles</strong>:</p>

<ul>
  <li>En GKE, Cilium se siente como una extensión natural de la plataforma</li>
  <li>En EKS, Cilium a menudo te obliga a revisar decisiones de red de bajo nivel</li>
</ul>

<p>Esto se nota especialmente cuando trabajas con:</p>
<ul>
  <li>Políticas FQDN</li>
  <li>Control avanzado de egress</li>
  <li>Integraciones con load balancers</li>
  <li>Clústeres con alta densidad de pods</li>
</ul>

<h2 id="perspectiva-operativa">Perspectiva operativa</h2>

<p>Desde el punto de vista de platform engineering, el soporte nativo de GKE para rangos secundarios elimina gran parte de la fricción operativa.</p>

<p>Permite:</p>
<ul>
  <li>Escalar clústeres sin pensar constantemente en límites de IP</li>
  <li>Aplicar políticas de red con menos sorpresas</li>
  <li>Centrarse en abstracciones de Kubernetes en lugar de soluciones específicas del proveedor cloud</li>
</ul>

<p>En EKS, los mismos resultados son posibles, pero normalmente requieren <strong>más planificación, más restricciones y mayor disciplina operativa</strong>.</p>

<h2 id="reflexión-final">Reflexión final</h2>

<p>Esto no es un debate GKE vs EKS.</p>

<p>Ambas plataformas están preparadas para producción y se usan ampliamente a gran escala.</p>

<p>Pero si tu plataforma depende fuertemente de networking avanzado en Kubernetes, <strong>el modelo de red de GKE se siente más cercano a cómo Kubernetes quiere ser operado</strong>, y eso se traduce directamente en una menor carga cognitiva para los equipos de plataforma.</p>

<p>A veces, menos sorpresas es la mejor característica.</p>

<p>Si estás diseñando o evolucionando una plataforma Kubernetes y quieres comentar estos trade-offs, puedes <a href="/es/contact">contactar conmigo</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Tras desplegar Cilium tanto en EKS como en GKE, una diferencia quedó muy clara: el networking es significativamente más fácil de razonar en GKE.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://plus.unsplash.com/premium_photo-1744345196324-94c618a49bc3?q=80&amp;w=1770&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.1.0&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D?w=800&amp;q=80" /><media:content medium="image" url="https://plus.unsplash.com/premium_photo-1744345196324-94c618a49bc3?q=80&amp;w=1770&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.1.0&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D?w=800&amp;q=80" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry xml:lang="en"><title type="html">Cilium on EKS vs GKE: why Kubernetes networking feels simpler on GKE</title><link href="http://localhost:4000/2026/01/08/cilium-networking-gke-vs-eks/" rel="alternate" type="text/html" title="Cilium on EKS vs GKE: why Kubernetes networking feels simpler on GKE" /><published>2026-01-08T00:00:00+01:00</published><updated>2026-01-08T00:00:00+01:00</updated><id>http://localhost:4000/2026/01/08/cilium-networking-gke-vs-eks</id><content type="html" xml:base="http://localhost:4000/2026/01/08/cilium-networking-gke-vs-eks/"><![CDATA[<p>After rolling out <strong>Cilium</strong> on both <strong>EKS</strong> and <strong>GKE</strong>, one thing became very clear to me:</p>

<p><strong>Networking is much easier to reason about on GKE.</strong></p>

<p>This is not about which platform is “better”, but about how closely the networking model aligns with how Kubernetes was originally designed to work.</p>

<h2 id="kubernetes-networking-as-designed-gke">Kubernetes networking as designed (GKE)</h2>

<p>On GKE, the Kubernetes networking model feels natural and predictable. Most things behave the way you expect them to if you understand Kubernetes fundamentals.</p>

<p>Some key characteristics:</p>

<ul>
  <li>Pods receive IPs from <strong>secondary VPC ranges</strong></li>
  <li><strong>IPAM can run in <code class="language-plaintext highlighter-rouge">kubernetes</code> mode</strong> without friction</li>
  <li>Pod IPs are <strong>first-class citizens</strong> in the VPC</li>
  <li>Pod density and scaling behave in a predictable way</li>
  <li>Advanced Cilium features work with minimal platform-specific adjustments</li>
</ul>

<p>From an operator point of view, this removes a lot of mental overhead. You reason in Kubernetes terms, not in infrastructure constraints.</p>

<h2 id="where-eks-adds-friction">Where EKS adds friction</h2>

<p>EKS is a solid platform, but networking is more tightly coupled to AWS infrastructure primitives.</p>

<p>In practice, this introduces additional constraints:</p>

<ul>
  <li>Pod IPs are tied to <strong>ENIs</strong></li>
  <li>IP exhaustion requires <strong>much more upfront planning</strong></li>
  <li>Pod density is constrained by EC2 instance types</li>
  <li>Some Cilium features require changing <strong>Load Balancer target types</strong> (instance vs IP)</li>
  <li>The mental model becomes <strong>infrastructure-driven</strong>, not Kubernetes-driven</li>
</ul>

<p>None of this makes EKS a bad platform. But it does mean that operating advanced networking setups requires deeper knowledge of AWS internals and more careful capacity planning.</p>

<h2 id="cilium-amplifies-the-differences">Cilium amplifies the differences</h2>

<p>Cilium itself is not the problem. In fact, it works very well on both platforms.</p>

<p>However, because Cilium exposes and relies on Kubernetes-native networking concepts, <strong>platform differences become more visible</strong>:</p>

<ul>
  <li>On GKE, Cilium feels like a natural extension of the platform</li>
  <li>On EKS, Cilium often forces you to revisit lower-level networking decisions</li>
</ul>

<p>This is especially noticeable when working with:</p>
<ul>
  <li>FQDN policies</li>
  <li>Advanced egress control</li>
  <li>Load balancer integrations</li>
  <li>High pod density clusters</li>
</ul>

<h2 id="operational-perspective">Operational perspective</h2>

<p>From a platform engineering point of view, GKE’s native support for secondary ranges removes a lot of operational friction.</p>

<p>It allows you to:</p>
<ul>
  <li>Scale clusters without constantly thinking about IP limits</li>
  <li>Apply network policies with fewer surprises</li>
  <li>Focus on Kubernetes abstractions instead of cloud-specific workarounds</li>
</ul>

<p>On EKS, the same outcomes are possible, but they usually require <strong>more planning, more constraints, and more operational discipline</strong>.</p>

<h2 id="final-thoughts">Final thoughts</h2>

<p>This is not a GKE vs EKS debate.</p>

<p>Both platforms are production-ready and widely used at scale.</p>

<p>But if your platform relies heavily on advanced Kubernetes networking, <strong>GKE’s networking model feels closer to how Kubernetes wants to be operated</strong>, and that translates directly into lower cognitive load for platform teams.</p>

<p>Sometimes, fewer surprises is the biggest feature.</p>

<p>If you’re designing or evolving a Kubernetes platform and want to discuss these trade-offs, feel free to <a href="/en/contact">get in touch</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[After rolling out Cilium on both EKS and GKE, one difference became very clear: networking is significantly easier to reason about on GKE.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://plus.unsplash.com/premium_photo-1744345196324-94c618a49bc3?q=80&amp;w=1770&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.1.0&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D?w=800&amp;q=80" /><media:content medium="image" url="https://plus.unsplash.com/premium_photo-1744345196324-94c618a49bc3?q=80&amp;w=1770&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.1.0&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D?w=800&amp;q=80" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>